{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# **Algorithm for Topic Extraction Using LDA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document present the process of development of an unsupervised algorithm for topic extraction. Currently the most reliable technic is the LDA (Latent Dirichlet Allocation) algorithm, which is base on the Dirichlet distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This notebook is meant to save the important notes and the project decisions. However, a \".py\" file containing the same code is available on this folder.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Approach**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The first step is to make the input of the data and the filter by language to avoid inconsistent results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05dfee830408402c8f8ab5160de79768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diego/Documentos/anaconda3/envs/i2a2/lib/python3.7/site-packages/tqdm/_tqdm.py:634: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "from Algorithms import preProcessing, modelUsageAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = ['datasets/articles_bbc_2018_01_30.csv', 'datasets/transcripts.csv', 'datasets/topics.csv']\n",
    "targets = ['articles','transcript', 'question_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset 0.\n",
      "\n",
      "Input Format:\n",
      "Rows: 309, Columns: 2.\n",
      "\n",
      "Pre cleaning format:\n",
      "Rows: 308, Columns: 2\n",
      "Loading the dataset 1.\n",
      "\n",
      "Input Format:\n",
      "Rows: 2467, Columns: 2.\n",
      "\n",
      "Pre cleaning format:\n",
      "Rows: 2467, Columns: 2\n",
      "Loading the dataset 2.\n",
      "\n",
      "Input Format:\n",
      "Rows: 5000, Columns: 3.\n",
      "\n",
      "Pre cleaning format:\n",
      "Rows: 5000, Columns: 3\n",
      "\n",
      "Removing unwanted information using targets.\n"
     ]
    }
   ],
   "source": [
    "data = preProcessing.import_files(paths, targets,\n",
    "                                   preCleaning = True,\n",
    "                                   dropna = 'index',\n",
    "                                   verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7775, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d970fe30aea46ada20e670076a7d489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7775), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "en    7723\n",
      "fa       9\n",
      "fr       8\n",
      "id       5\n",
      "uk       4\n",
      "vi       4\n",
      "ar       4\n",
      "ru       4\n",
      "hi       4\n",
      "sw       3\n",
      "es       2\n",
      "pt       2\n",
      "tr       2\n",
      "de       1\n",
      "Name: lang, dtype: int64\n",
      "\n",
      "Most Frequent language: en    7723\n",
      "Name: lang, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "en    7723\n",
       "Name: lang, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preProcessing.language_detection(data,\n",
    "                                 verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning data using 'en' language.\n"
     ]
    }
   ],
   "source": [
    "data = preProcessing.language_cleaning(dataFile = data,\n",
    "                                        language = 'en',\n",
    "                                        verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Processing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Tokenizing** the documents to the word level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing data to word\n",
      ".\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc436e151cda44eea8d6d75cc640badc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7723), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3009c298c40548c7a37d48fcd1e8e04e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7723), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = preProcessing.tokenization(data, level = 'word', verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing for **Lemmatizizing** using POS tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2456790be68949279190ff8cd5b981f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7723), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = preProcessing.POS_tagging(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92081c70d0b64b8388bc6b50e5a33822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7723), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = preProcessing.lemmatizing(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing **StopWords** using the english stopwords from the Natural Language Toolkit (NLTK) and removing any token less than 2 characthers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preProcessing.removeStopWords(data, minSize = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainning the LDA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating the tokens using the Bigram and the Trigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tokens From data file and converting into a list of tokens.\n",
      "Building the Bigram Model\n",
      "Building the Trigram Model\n",
      "Importing the Trigram Model and converting into list\n"
     ]
    }
   ],
   "source": [
    "tokens = preProcessing.Bi_n_TrigramModel(data, min_cnt = 1, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the dictionary using the the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating the Ditionary.\n",
      "Filtering dictionary using the minimun threshold: 3\n"
     ]
    }
   ],
   "source": [
    "dictionary = preProcessing.generateDictionary(tokens, min_thld = 3, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating the BOW for the LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Bag Of Words.\n"
     ]
    }
   ],
   "source": [
    "bow = preProcessing.generateBOW(dictionary, tokens, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating the LDA based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainning LDA model using the inputed BOW and Dictionary.\n",
      "Parameters: Topics: 16, Passes: 4\n",
      "CPU times: user 1min 39s, sys: 55.5 s, total: 2min 35s\n",
      "Wall time: 56.1 s\n"
     ]
    }
   ],
   "source": [
    "numberOfTopics = 16\n",
    "%time LDAModel = preProcessing.trainModel(bow, dictionary, numTopics = numberOfTopics, numPasses = 4, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A quick view on the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.020*\"think\" + 0.016*\"actually\" + 0.014*\"thing\" + 0.012*\"look\" + 0.009*\"right\" + 0.008*\"way\" + 0.008*\"laughter\" + 0.008*\"kind\" + 0.006*\"work\" + 0.006*\"brain\" + 0.006*\"lot\" + 0.005*\"people\" + 0.005*\"two\" + 0.005*\"good\" + 0.005*\"show\" + 0.005*\"find\" + 0.005*\"time\" + 0.005*\"happen\" + 0.005*\"sort\" + 0.005*\"tell\"\n",
      "\n",
      "1: 0.016*\"people\" + 0.009*\"think\" + 0.008*\"work\" + 0.007*\"look\" + 0.007*\"time\" + 0.007*\"thing\" + 0.006*\"good\" + 0.006*\"way\" + 0.006*\"percent\" + 0.005*\"country\" + 0.005*\"woman\" + 0.005*\"start\" + 0.005*\"many\" + 0.004*\"year\" + 0.004*\"give\" + 0.004*\"world\" + 0.004*\"could\" + 0.004*\"find\" + 0.004*\"first\" + 0.004*\"actually\"\n",
      "\n",
      "2: 0.010*\"laughter\" + 0.009*\"think\" + 0.009*\"look\" + 0.008*\"life\" + 0.008*\"time\" + 0.007*\"could\" + 0.007*\"thing\" + 0.006*\"back\" + 0.006*\"way\" + 0.005*\"find\" + 0.005*\"day\" + 0.005*\"woman\" + 0.005*\"call\" + 0.005*\"start\" + 0.005*\"people\" + 0.005*\"work\" + 0.005*\"tell\" + 0.004*\"right\" + 0.004*\"talk\" + 0.004*\"first\"\n",
      "\n",
      "3: 0.015*\"world\" + 0.013*\"country\" + 0.010*\"people\" + 0.010*\"think\" + 0.008*\"could\" + 0.008*\"china\" + 0.006*\"new\" + 0.006*\"power\" + 0.006*\"happen\" + 0.006*\"build\" + 0.006*\"thing\" + 0.006*\"today\" + 0.006*\"way\" + 0.005*\"create\" + 0.005*\"economy\" + 0.005*\"call\" + 0.004*\"india\" + 0.004*\"actually\" + 0.004*\"building\" + 0.004*\"global\"\n",
      "\n",
      "4: 0.018*\"laughter\" + 0.015*\"think\" + 0.012*\"thing\" + 0.010*\"people\" + 0.010*\"music\" + 0.009*\"look\" + 0.008*\"right\" + 0.008*\"start\" + 0.008*\"way\" + 0.008*\"work\" + 0.008*\"play\" + 0.007*\"tell\" + 0.007*\"robot\" + 0.007*\"could\" + 0.007*\"sound\" + 0.006*\"time\" + 0.006*\"actually\" + 0.006*\"learn\" + 0.006*\"let\" + 0.006*\"try\"\n",
      "\n",
      "5: 0.066*\"brain\" + 0.017*\"body\" + 0.009*\"different\" + 0.009*\"animal\" + 0.008*\"neuron\" + 0.008*\"control\" + 0.007*\"cell\" + 0.007*\"model\" + 0.007*\"face\" + 0.006*\"time\" + 0.006*\"first\" + 0.006*\"look\" + 0.006*\"move\" + 0.006*\"think\" + 0.005*\"show\" + 0.005*\"experience\" + 0.005*\"monkey\" + 0.005*\"happen\" + 0.005*\"laptop\" + 0.005*\"eye\"\n",
      "\n",
      "6: 0.066*\"store\" + 0.044*\"check\" + 0.040*\"dimension\" + 0.033*\"chance\" + 0.032*\"shoe\" + 0.032*\"bag\" + 0.032*\"could\" + 0.027*\"think\" + 0.025*\"person\" + 0.023*\"short\" + 0.022*\"zip\" + 0.020*\"purchase\" + 0.015*\"interested\" + 0.014*\"tell\" + 0.014*\"alzheimer\" + 0.013*\"case\" + 0.012*\"charge\" + 0.012*\"price\" + 0.011*\"look\" + 0.011*\"realize\"\n",
      "\n",
      "7: 0.119*\"item\" + 0.063*\"look\" + 0.030*\"buy\" + 0.029*\"product\" + 0.025*\"could\" + 0.018*\"tell\" + 0.017*\"long\" + 0.016*\"think\" + 0.013*\"code\" + 0.013*\"sell\" + 0.012*\"wonder\" + 0.011*\"leather\" + 0.011*\"battery\" + 0.010*\"protein\" + 0.010*\"men\" + 0.008*\"powder\" + 0.008*\"mail\" + 0.007*\"balance\" + 0.007*\"type\" + 0.007*\"thing\"\n",
      "\n",
      "8: 0.020*\"people\" + 0.012*\"think\" + 0.010*\"thing\" + 0.007*\"way\" + 0.006*\"time\" + 0.005*\"right\" + 0.005*\"world\" + 0.005*\"many\" + 0.005*\"laughter\" + 0.005*\"mean\" + 0.005*\"good\" + 0.005*\"life\" + 0.004*\"talk\" + 0.004*\"give\" + 0.004*\"could\" + 0.004*\"actually\" + 0.004*\"look\" + 0.004*\"happen\" + 0.004*\"work\" + 0.004*\"change\"\n",
      "\n",
      "9: 0.044*\"style\" + 0.038*\"happen\" + 0.029*\"offer\" + 0.026*\"jacket\" + 0.020*\"guy\" + 0.019*\"woman\" + 0.019*\"kind\" + 0.018*\"look\" + 0.016*\"discount\" + 0.015*\"worry\" + 0.015*\"dress\" + 0.013*\"section_website\" + 0.013*\"buy\" + 0.012*\"gold\" + 0.011*\"think\" + 0.011*\"cotton\" + 0.010*\"wife\" + 0.010*\"cartoon\" + 0.009*\"blue\" + 0.009*\"help\"\n",
      "\n",
      "10: 0.021*\"city\" + 0.015*\"people\" + 0.011*\"look\" + 0.010*\"think\" + 0.009*\"right\" + 0.009*\"thing\" + 0.008*\"car\" + 0.008*\"build\" + 0.008*\"start\" + 0.008*\"could\" + 0.007*\"work\" + 0.006*\"lot\" + 0.006*\"place\" + 0.006*\"way\" + 0.005*\"actually\" + 0.005*\"kind\" + 0.005*\"back\" + 0.005*\"put\" + 0.005*\"find\" + 0.004*\"time\"\n",
      "\n",
      "11: 0.012*\"work\" + 0.011*\"thing\" + 0.009*\"could\" + 0.009*\"people\" + 0.008*\"actually\" + 0.007*\"think\" + 0.007*\"design\" + 0.007*\"start\" + 0.006*\"way\" + 0.005*\"kind\" + 0.005*\"time\" + 0.005*\"first\" + 0.005*\"call\" + 0.005*\"look\" + 0.005*\"new\" + 0.004*\"patient\" + 0.004*\"laughter\" + 0.004*\"idea\" + 0.004*\"technology\" + 0.004*\"right\"\n",
      "\n",
      "12: 0.102*\"order\" + 0.051*\"think\" + 0.050*\"could\" + 0.049*\"look\" + 0.033*\"either\" + 0.027*\"receive\" + 0.024*\"return\" + 0.021*\"website\" + 0.021*\"size\" + 0.019*\"able\" + 0.018*\"site\" + 0.014*\"try\" + 0.013*\"tell\" + 0.012*\"decide\" + 0.011*\"online\" + 0.011*\"ship\" + 0.010*\"option\" + 0.010*\"sale\" + 0.010*\"anything\" + 0.009*\"sure\"\n",
      "\n",
      "13: 0.008*\"world\" + 0.007*\"people\" + 0.007*\"look\" + 0.007*\"think\" + 0.006*\"time\" + 0.005*\"find\" + 0.005*\"could\" + 0.004*\"way\" + 0.004*\"thing\" + 0.004*\"year\" + 0.004*\"water\" + 0.004*\"first\" + 0.004*\"many\" + 0.004*\"laughter\" + 0.004*\"right\" + 0.003*\"grow\" + 0.003*\"back\" + 0.003*\"talk\" + 0.003*\"live\" + 0.003*\"work\"\n",
      "\n",
      "14: 0.020*\"wonder\" + 0.018*\"map\" + 0.017*\"find\" + 0.015*\"large\" + 0.012*\"boot\" + 0.012*\"cave\" + 0.010*\"tree\" + 0.009*\"planet\" + 0.009*\"place\" + 0.008*\"area\" + 0.008*\"black\" + 0.008*\"earth\" + 0.008*\"water\" + 0.007*\"could\" + 0.007*\"kind_material\" + 0.007*\"life\" + 0.007*\"world\" + 0.006*\"river\" + 0.006*\"heel\" + 0.006*\"look\"\n",
      "\n",
      "15: 0.018*\"look\" + 0.009*\"small\" + 0.009*\"universe\" + 0.009*\"planet\" + 0.008*\"actually\" + 0.008*\"could\" + 0.008*\"space\" + 0.008*\"time\" + 0.008*\"light\" + 0.008*\"thing\" + 0.007*\"earth\" + 0.007*\"way\" + 0.006*\"find\" + 0.006*\"think\" + 0.006*\"big\" + 0.006*\"star\" + 0.005*\"around\" + 0.005*\"show\" + 0.004*\"picture\" + 0.004*\"work\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,topic in LDAModel.show_topics(formatted=True, num_topics = numberOfTopics, num_words=20):\n",
    "    print(str(i)+\": \"+ topic)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelUsageAPI.save(LDAModel,'models/LDAmodelExtended.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validating the model on the document in position 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbDoc = 0\n",
    "valDoc = data.articles.loc[nbDoc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image copyright PA/EPA Image caption Oligarch Roman Abramovich (l) and PM Dmitry Medvedev are on the list\n",
      "\n",
      "Russian President Vladimir Putin says a list of officials and businessmen close to the Kremlin published by the US has in effect targeted all Russian people.\n",
      "\n",
      "The list names 210 top Russians as part of a sanctions law aimed at punishing Moscow for meddling in the US election.\n",
      "\n",
      "However, the US stressed those named were not subject to new sanctions.\n",
      "\n",
      "Mr Putin said the list was an unfr\n"
     ]
    }
   ],
   "source": [
    "print(valDoc[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 0.14982004), (3, 0.30157804), (8, 0.37452465), (12, 0.17343038)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDAModel[bow[nbDoc]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
