{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# **Algorithm for Topic Extraction Using LDA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document present the process of development of an unsupervised algorithm for topic extraction. Currently the most reliable technic is the LDA (Latent Dirichlet Allocation) algorithm, which is base on the Dirichlet distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This notebook is meant to save the important notes and the project decisions. However, a \".py\" file containing the same code is available on this folder.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Approach**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The first step is to make the input of the data and the filter by language to avoid inconsistent results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f9ff405fa0a4a738937623c7ca173bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diego/Documentos/anaconda3/envs/i2a2/lib/python3.7/site-packages/tqdm/_tqdm.py:634: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "from Algorithms import preProcessing_BBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Format:\n",
      "Rows: 309, Columns: 2\n",
      "\n",
      "Pre cleaning format:\n",
      "Rows: 308, Columns: 2\n"
     ]
    }
   ],
   "source": [
    "data = preProcessing_BBC.import_files(\"articles_bbc_2018_01_30.csv\",\n",
    "                                   preCleaning = True,\n",
    "                                   dropna = 'index',\n",
    "                                   verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fb31d26adc24084bc53baae76665350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=308), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "en    257\n",
      "fa      9\n",
      "fr      7\n",
      "id      5\n",
      "vi      4\n",
      "ru      4\n",
      "hi      4\n",
      "uk      4\n",
      "ar      4\n",
      "sw      3\n",
      "tr      2\n",
      "pt      2\n",
      "es      2\n",
      "de      1\n",
      "Name: lang, dtype: int64\n",
      "\n",
      "Most Frequent language: en    257\n",
      "Name: lang, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "en    257\n",
       "Name: lang, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preProcessing_BBC.language_detection(data,\n",
    "                                 verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning data using 'en' language.\n"
     ]
    }
   ],
   "source": [
    "data = preProcessing_BBC.language_cleaning(dataFile = data,\n",
    "                                        language = 'en',\n",
    "                                        verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Processing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Tokenizing** the documents to the word level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing data to word\n",
      ".\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d225c58d8f24246bf466759657e2c14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=257), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "402d6b3ae202417a91dc7c011dd3f35b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=257), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = preProcessing_BBC.tokenization(data, level = 'word', verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing for **Lemmatizizing** using POS tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed273d33ef9d4430bc7eaa2acf1ab3ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=257), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = preProcessing_BBC.POS_tagging(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd329f6a22a741f285797517d801e663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=257), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = preProcessing_BBC.lemmatizing(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing **StopWords** using the english stopwords from the Natural Language Toolkit (NLTK) and removing any token less than 2 characthers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preProcessing_BBC.removeStopWords(data, minSize = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Trainning the LDA Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating the tokens using the Bigram and the Trigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tokens From data file and converting into a list of tokens.\n",
      "Building the Bigram Model\n",
      "Building the Trigram Model\n",
      "Importing the Trigram Model and converting into list\n"
     ]
    }
   ],
   "source": [
    "tokens = preProcessing_BBC.Bi_n_TrigramModel(data, min_cnt = 1, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the dictionary using the the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating the Ditionary.\n",
      "Filtering dictionary using the minimun threshold: 3\n"
     ]
    }
   ],
   "source": [
    "dictionary = preProcessing_BBC.generateDictionary(tokens, min_thld = 3, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating the BOW for the LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Bag Of Words.\n"
     ]
    }
   ],
   "source": [
    "bow = preProcessing_BBC.generateBOW(dictionary, tokens, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating the LDA based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainning LDA model using the inputed BOW and Dictionary.\n",
      "Parameters: Topics: 20, Passes: 4\n",
      "CPU times: user 1.75 s, sys: 0 ns, total: 1.75 s\n",
      "Wall time: 1.75 s\n"
     ]
    }
   ],
   "source": [
    "%time LDAModel = preProcessing_BBC.trainModel(bow, dictionary, numTopics = 20, numPasses = 4, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.016*\"africa\" + 0.014*\"find\" + 0.012*\"animal\" + 0.012*\"light\" + 0.009*\"news\" + 0.009*\"specie\" + 0.008*\"continent\" + 0.008*\"long\" + 0.008*\"much\" + 0.008*\"help\" + 0.008*\"transparent\" + 0.008*\"twitter\" + 0.007*\"rather\" + 0.007*\"time\" + 0.007*\"giant\" + 0.006*\"predator\" + 0.006*\"plate\" + 0.005*\"rock\" + 0.005*\"study\" + 0.005*\"live\"\n",
      "\n",
      "1: 0.008*\"water\" + 0.008*\"wave\" + 0.008*\"ocean\" + 0.008*\"find\" + 0.006*\"animal\" + 0.006*\"night\" + 0.006*\"world\" + 0.005*\"northern_mali\" + 0.005*\"light\" + 0.005*\"group\" + 0.005*\"area\" + 0.005*\"logo\" + 0.004*\"call\" + 0.004*\"create\" + 0.004*\"day\" + 0.004*\"country\" + 0.004*\"seal\" + 0.004*\"include\" + 0.004*\"healthcare\" + 0.004*\"design\"\n",
      "\n",
      "2: 0.011*\"radio\" + 0.011*\"case\" + 0.010*\"ansar_dine\" + 0.009*\"people\" + 0.009*\"review\" + 0.008*\"subject\" + 0.008*\"find\" + 0.008*\"tuareg\" + 0.007*\"report\" + 0.007*\"university\" + 0.006*\"think\" + 0.006*\"digital\" + 0.006*\"tell\" + 0.006*\"add\" + 0.006*\"head\" + 0.006*\"saudi_arabia\" + 0.006*\"government\" + 0.006*\"call\" + 0.005*\"last_year\" + 0.005*\"wear\"\n",
      "\n",
      "3: 0.011*\"instagram\" + 0.009*\"time\" + 0.009*\"designer\" + 0.009*\"amazon\" + 0.008*\"photograph\" + 0.008*\"design\" + 0.007*\"town\" + 0.006*\"store\" + 0.006*\"year\" + 0.006*\"photographer\" + 0.006*\"help\" + 0.006*\"community\" + 0.006*\"item\" + 0.006*\"visual\" + 0.006*\"first\" + 0.005*\"return\" + 0.005*\"new\" + 0.005*\"put\" + 0.005*\"place\" + 0.005*\"work\"\n",
      "\n",
      "4: 0.021*\"village\" + 0.012*\"sleep\" + 0.010*\"good\" + 0.009*\"work\" + 0.009*\"people\" + 0.008*\"house\" + 0.008*\"way\" + 0.007*\"well\" + 0.007*\"shy\" + 0.006*\"level\" + 0.006*\"cafe\" + 0.006*\"stop\" + 0.006*\"even\" + 0.006*\"mountain\" + 0.005*\"roof\" + 0.005*\"accord\" + 0.005*\"cognitive\" + 0.005*\"wind\" + 0.005*\"productivity\" + 0.005*\"tea\"\n",
      "\n",
      "5: 0.014*\"film\" + 0.009*\"prey\" + 0.009*\"write\" + 0.009*\"store\" + 0.008*\"work\" + 0.008*\"show\" + 0.007*\"enjoy\" + 0.007*\"move\" + 0.006*\"think\" + 0.006*\"part\" + 0.006*\"people\" + 0.006*\"support\" + 0.006*\"actress\" + 0.006*\"year\" + 0.005*\"interview\" + 0.005*\"idea\" + 0.005*\"video\" + 0.005*\"nothing\" + 0.005*\"turn\" + 0.005*\"else\"\n",
      "\n",
      "6: 0.006*\"first\" + 0.006*\"work\" + 0.006*\"time\" + 0.005*\"people\" + 0.005*\"find\" + 0.005*\"could\" + 0.005*\"back\" + 0.005*\"company\" + 0.005*\"look\" + 0.004*\"dutch\" + 0.004*\"much\" + 0.004*\"part\" + 0.004*\"even\" + 0.004*\"place\" + 0.004*\"call\" + 0.004*\"day\" + 0.003*\"tunnel\" + 0.003*\"child\" + 0.003*\"building\" + 0.003*\"city\"\n",
      "\n",
      "7: 0.016*\"show\" + 0.013*\"people\" + 0.011*\"president\" + 0.010*\"loan\" + 0.008*\"photo\" + 0.007*\"win\" + 0.007*\"moment\" + 0.007*\"election\" + 0.007*\"government\" + 0.006*\"judge\" + 0.006*\"tell\" + 0.005*\"man\" + 0.005*\"general\" + 0.005*\"photograph\" + 0.005*\"air\" + 0.005*\"event\" + 0.004*\"wood\" + 0.004*\"meeting\" + 0.004*\"series\" + 0.004*\"ban\"\n",
      "\n",
      "8: 0.016*\"question\" + 0.015*\"transparent\" + 0.011*\"however\" + 0.010*\"student\" + 0.010*\"find\" + 0.009*\"live\" + 0.008*\"light\" + 0.008*\"call\" + 0.007*\"chinese\" + 0.007*\"hide\" + 0.007*\"eye\" + 0.007*\"another\" + 0.007*\"think\" + 0.007*\"school\" + 0.006*\"predator\" + 0.006*\"captain\" + 0.006*\"visible\" + 0.005*\"task\" + 0.005*\"sally\" + 0.005*\"least\"\n",
      "\n",
      "9: 0.009*\"people\" + 0.006*\"government\" + 0.006*\"year\" + 0.005*\"back\" + 0.005*\"show\" + 0.005*\"city\" + 0.004*\"country\" + 0.004*\"many\" + 0.004*\"support\" + 0.004*\"images\" + 0.004*\"time\" + 0.004*\"first\" + 0.004*\"party\" + 0.004*\"bring\" + 0.004*\"separatist\" + 0.004*\"president\" + 0.004*\"include\" + 0.003*\"give\" + 0.003*\"think\" + 0.003*\"political\"\n",
      "\n",
      "10: 0.013*\"dish\" + 0.010*\"call\" + 0.007*\"celebration\" + 0.007*\"england\" + 0.007*\"ground\" + 0.006*\"kill\" + 0.006*\"hold\" + 0.006*\"artist\" + 0.006*\"back\" + 0.006*\"ask\" + 0.006*\"play\" + 0.006*\"syria\" + 0.006*\"club\" + 0.005*\"animal\" + 0.005*\"small\" + 0.005*\"former\" + 0.005*\"edge\" + 0.005*\"world\" + 0.005*\"ancient\" + 0.005*\"http\"\n",
      "\n",
      "11: 0.010*\"think\" + 0.009*\"news\" + 0.009*\"work\" + 0.008*\"people\" + 0.007*\"find\" + 0.007*\"die\" + 0.007*\"story\" + 0.006*\"report\" + 0.005*\"really\" + 0.005*\"job\" + 0.005*\"output\" + 0.005*\"link\" + 0.005*\"include\" + 0.004*\"model\" + 0.004*\"journalism\" + 0.004*\"age\" + 0.004*\"way\" + 0.004*\"body\" + 0.004*\"set\" + 0.004*\"could\"\n",
      "\n",
      "12: 0.007*\"work\" + 0.006*\"find\" + 0.006*\"people\" + 0.005*\"could\" + 0.005*\"report\" + 0.005*\"day\" + 0.004*\"tell\" + 0.004*\"specie\" + 0.004*\"call\" + 0.004*\"system\" + 0.004*\"help\" + 0.004*\"university\" + 0.004*\"study\" + 0.004*\"number\" + 0.004*\"even\" + 0.004*\"images\" + 0.003*\"body\" + 0.003*\"attack\" + 0.003*\"lead\" + 0.003*\"another\"\n",
      "\n",
      "13: 0.016*\"video\" + 0.012*\"platform\" + 0.010*\"satellite\" + 0.010*\"rocket\" + 0.008*\"flight\" + 0.008*\"judge\" + 0.007*\"time\" + 0.006*\"app\" + 0.006*\"jail\" + 0.005*\"good\" + 0.005*\"include\" + 0.005*\"benefit\" + 0.005*\"news_app\" + 0.005*\"home\" + 0.005*\"reach\" + 0.005*\"sentence\" + 0.005*\"six\" + 0.005*\"raise\" + 0.005*\"vehicle\" + 0.005*\"criminal\"\n",
      "\n",
      "14: 0.017*\"idea\" + 0.009*\"pick\" + 0.009*\"spanish\" + 0.008*\"year\" + 0.007*\"work\" + 0.007*\"note\" + 0.007*\"help\" + 0.007*\"call\" + 0.006*\"new\" + 0.006*\"write\" + 0.006*\"funny\" + 0.006*\"script\" + 0.006*\"world\" + 0.006*\"step\" + 0.006*\"emotion\" + 0.006*\"doctor\" + 0.006*\"list\" + 0.006*\"usually\" + 0.006*\"please\" + 0.006*\"simple\"\n",
      "\n",
      "15: 0.010*\"money\" + 0.010*\"prize\" + 0.010*\"asset\" + 0.010*\"producer\" + 0.009*\"help\" + 0.009*\"award\" + 0.009*\"give\" + 0.008*\"last_year\" + 0.008*\"drug\" + 0.008*\"look\" + 0.007*\"pick\" + 0.007*\"drive\" + 0.007*\"back\" + 0.007*\"fund\" + 0.006*\"global\" + 0.006*\"grow\" + 0.006*\"work\" + 0.006*\"rate\" + 0.006*\"woman\" + 0.006*\"year\"\n",
      "\n",
      "16: 0.013*\"base\" + 0.011*\"could\" + 0.010*\"data\" + 0.009*\"military\" + 0.008*\"show\" + 0.008*\"design\" + 0.008*\"run\" + 0.007*\"work\" + 0.007*\"security\" + 0.007*\"test\" + 0.006*\"images\" + 0.006*\"soldier\" + 0.006*\"time\" + 0.006*\"people\" + 0.006*\"two\" + 0.006*\"call\" + 0.005*\"ban\" + 0.005*\"software\" + 0.005*\"activity\" + 0.005*\"tell\"\n",
      "\n",
      "17: 0.011*\"light\" + 0.008*\"russia\" + 0.008*\"list\" + 0.007*\"day\" + 0.007*\"people\" + 0.007*\"report\" + 0.006*\"russian\" + 0.006*\"even\" + 0.006*\"first\" + 0.006*\"feel\" + 0.006*\"president\" + 0.006*\"time\" + 0.006*\"work\" + 0.005*\"tell\" + 0.005*\"think\" + 0.005*\"name\" + 0.005*\"play\" + 0.005*\"brexit\" + 0.005*\"leave\" + 0.005*\"could\"\n",
      "\n",
      "18: 0.011*\"chris\" + 0.009*\"lion\" + 0.008*\"hunter\" + 0.008*\"home\" + 0.008*\"first\" + 0.007*\"video\" + 0.007*\"girl\" + 0.007*\"south_africa\" + 0.007*\"live\" + 0.007*\"small\" + 0.007*\"travel\" + 0.006*\"police\" + 0.006*\"help_stay\" + 0.006*\"grow_body\" + 0.006*\"kill\" + 0.006*\"search\" + 0.006*\"work\" + 0.005*\"time\" + 0.005*\"life\" + 0.005*\"look\"\n",
      "\n",
      "19: 0.013*\"game\" + 0.011*\"restaurant\" + 0.009*\"drug\" + 0.009*\"study\" + 0.008*\"star\" + 0.008*\"health\" + 0.008*\"rating\" + 0.008*\"part\" + 0.007*\"group\" + 0.007*\"organisation\" + 0.006*\"human\" + 0.006*\"play\" + 0.006*\"plan\" + 0.006*\"could\" + 0.005*\"give\" + 0.005*\"include\" + 0.005*\"case\" + 0.005*\"release\" + 0.005*\"publish\" + 0.005*\"control\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,topic in LDAModel.show_topics(formatted=True, num_topics=20, num_words=20):\n",
    "    print(str(i)+\": \"+ topic)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
