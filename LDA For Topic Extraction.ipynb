{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# **Algorithm for Topic Extraction Using LDA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document present the process of development of an unsupervised algorithm for topic extraction. Currently the most reliable technic is the LDA (Latent Dirichlet Allocation) algorithm, which is base on the Dirichlet distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This notebook is meant to save the important notes and the project decisions. However, a \".py\" file containing the same code is available on this folder.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Approach**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The first step is to make the input of the data and the filter by language to avoid inconsistent results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd9b40c7857a491289b8171c585ebea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diego/Documentos/anaconda3/envs/i2a2/lib/python3.7/site-packages/tqdm/_tqdm.py:634: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "from Algorithms import preProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = ['datasets/articles_bbc_2018_01_30.csv', 'datasets/transcripts.csv']\n",
    "targets = ['articles','transcript']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset 0.\n",
      "\n",
      "Input Format:\n",
      "Rows: 309, Columns: 2.\n",
      "\n",
      "Pre cleaning format:\n",
      "Rows: 308, Columns: 2\n",
      "Loading the dataset 1.\n",
      "\n",
      "Input Format:\n",
      "Rows: 2467, Columns: 2.\n",
      "\n",
      "Pre cleaning format:\n",
      "Rows: 2467, Columns: 2\n",
      "Removing unwanted information using targets.\n"
     ]
    }
   ],
   "source": [
    "data = preProcessing.import_files(paths, targets,\n",
    "                                   preCleaning = True,\n",
    "                                   dropna = 'index',\n",
    "                                   verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "270fcab3f36943dca0091848cd018932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2775), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "en    2723\n",
      "fa       9\n",
      "fr       8\n",
      "id       5\n",
      "vi       4\n",
      "uk       4\n",
      "hi       4\n",
      "ar       4\n",
      "ru       4\n",
      "sw       3\n",
      "pt       2\n",
      "es       2\n",
      "tr       2\n",
      "de       1\n",
      "Name: lang, dtype: int64\n",
      "\n",
      "Most Frequent language: en    2723\n",
      "Name: lang, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "en    2723\n",
       "Name: lang, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preProcessing.language_detection(data,\n",
    "                                 verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning data using 'en' language.\n"
     ]
    }
   ],
   "source": [
    "data = preProcessing.language_cleaning(dataFile = data,\n",
    "                                        language = 'en',\n",
    "                                        verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Processing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Tokenizing** the documents to the word level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing data to word\n",
      ".\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02443b7549c7423e89d09dee384fc130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2723), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d53b7c73cfe4cadaa44cbbe63aff557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2723), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = preProcessing.tokenization(data, level = 'word', verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing for **Lemmatizizing** using POS tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f84fb9a9af2f4815a69a8404c5083568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2723), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = preProcessing.POS_tagging(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c872096a936047e2acc49fb254c95538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2723), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = preProcessing.lemmatizing(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing **StopWords** using the english stopwords from the Natural Language Toolkit (NLTK) and removing any token less than 2 characthers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preProcessing.removeStopWords(data, minSize = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainning the LDA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating the tokens using the Bigram and the Trigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tokens From data file and converting into a list of tokens.\n",
      "Building the Bigram Model\n",
      "Building the Trigram Model\n",
      "Importing the Trigram Model and converting into list\n"
     ]
    }
   ],
   "source": [
    "tokens = preProcessing.Bi_n_TrigramModel(data, min_cnt = 1, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the dictionary using the the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating the Ditionary.\n",
      "Filtering dictionary using the minimun threshold: 3\n"
     ]
    }
   ],
   "source": [
    "dictionary = preProcessing.generateDictionary(tokens, min_thld = 3, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating the BOW for the LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Bag Of Words.\n"
     ]
    }
   ],
   "source": [
    "bow = preProcessing.generateBOW(dictionary, tokens, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating the LDA based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainning LDA model using the inputed BOW and Dictionary.\n",
      "Parameters: Topics: 20, Passes: 4\n",
      "CPU times: user 2min 35s, sys: 1min 37s, total: 4min 12s\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "numberOfTopics = 20\n",
    "%time LDAModel = preProcessing.trainModel(bow, dictionary, numTopics = numberOfTopics, numPasses = 4, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.062*\"love\" + 0.021*\"sleep\" + 0.012*\"question\" + 0.011*\"romantic_love\" + 0.008*\"relationship\" + 0.008*\"study\" + 0.007*\"metaphor\" + 0.006*\"fall_love\" + 0.006*\"golf\" + 0.005*\"teen\" + 0.005*\"choice\" + 0.005*\"hummus\" + 0.004*\"feel\" + 0.004*\"teenager\" + 0.004*\"understand\" + 0.004*\"animal\" + 0.004*\"environmental_protection\" + 0.003*\"someone\" + 0.003*\"song\" + 0.003*\"article\"\n",
      "\n",
      "1: 0.008*\"brain\" + 0.006*\"story\" + 0.005*\"face\" + 0.004*\"understand\" + 0.004*\"help\" + 0.004*\"feel\" + 0.004*\"study\" + 0.004*\"case\" + 0.004*\"human\" + 0.004*\"reason\" + 0.003*\"might\" + 0.003*\"fear\" + 0.003*\"percent\" + 0.003*\"decision\" + 0.003*\"example\" + 0.003*\"learn\" + 0.003*\"behavior\" + 0.003*\"maybe\" + 0.003*\"system\" + 0.003*\"bad\"\n",
      "\n",
      "2: 0.009*\"country\" + 0.004*\"government\" + 0.004*\"problem\" + 0.004*\"human\" + 0.004*\"percent\" + 0.003*\"create\" + 0.003*\"question\" + 0.003*\"believe\" + 0.003*\"help\" + 0.003*\"learn\" + 0.003*\"system\" + 0.003*\"important\" + 0.003*\"example\" + 0.003*\"number\" + 0.003*\"job\" + 0.002*\"society\" + 0.002*\"money\" + 0.002*\"child\" + 0.002*\"technology\" + 0.002*\"understand\"\n",
      "\n",
      "3: 0.007*\"feel\" + 0.007*\"experience\" + 0.005*\"love\" + 0.004*\"never\" + 0.004*\"brain\" + 0.004*\"learn\" + 0.004*\"body\" + 0.003*\"always\" + 0.003*\"moment\" + 0.003*\"help\" + 0.003*\"human\" + 0.003*\"mind\" + 0.003*\"play\" + 0.003*\"remember\" + 0.003*\"child\" + 0.003*\"story\" + 0.003*\"everything\" + 0.003*\"person\" + 0.003*\"might\" + 0.003*\"game\"\n",
      "\n",
      "4: 0.020*\"data\" + 0.016*\"body\" + 0.009*\"death\" + 0.008*\"big_data\" + 0.006*\"die\" + 0.005*\"apple\" + 0.005*\"breast_milk\" + 0.005*\"compost\" + 0.005*\"dolphin\" + 0.005*\"pancreatic_cancer\" + 0.005*\"milk\" + 0.005*\"cemetery\" + 0.004*\"play\" + 0.004*\"family\" + 0.004*\"lab\" + 0.004*\"matcher\" + 0.004*\"pdf\" + 0.004*\"hand\" + 0.004*\"car\" + 0.004*\"caitlin\"\n",
      "\n",
      "5: 0.005*\"community\" + 0.004*\"help\" + 0.004*\"create\" + 0.003*\"city\" + 0.003*\"design\" + 0.003*\"love\" + 0.003*\"share\" + 0.003*\"feel\" + 0.003*\"believe\" + 0.003*\"home\" + 0.003*\"project\" + 0.003*\"learn\" + 0.003*\"always\" + 0.003*\"together\" + 0.003*\"bring\" + 0.003*\"never\" + 0.002*\"every\" + 0.002*\"experience\" + 0.002*\"story\" + 0.002*\"decide\"\n",
      "\n",
      "6: 0.012*\"mars\" + 0.008*\"ocean\" + 0.007*\"earth\" + 0.006*\"drug\" + 0.006*\"planet\" + 0.006*\"patient\" + 0.005*\"percent\" + 0.004*\"brain\" + 0.004*\"climate_change\" + 0.004*\"data\" + 0.004*\"understand\" + 0.004*\"problem\" + 0.004*\"climate\" + 0.004*\"ice\" + 0.003*\"move\" + 0.003*\"able\" + 0.003*\"energy\" + 0.003*\"animal\" + 0.003*\"study\" + 0.003*\"human\"\n",
      "\n",
      "7: 0.016*\"student\" + 0.010*\"company\" + 0.008*\"machine\" + 0.007*\"school\" + 0.007*\"problem\" + 0.007*\"learn\" + 0.006*\"computer\" + 0.006*\"teacher\" + 0.006*\"technology\" + 0.006*\"course\" + 0.006*\"teach\" + 0.005*\"question\" + 0.005*\"example\" + 0.005*\"percent\" + 0.005*\"build\" + 0.005*\"maybe\" + 0.005*\"product\" + 0.004*\"data\" + 0.004*\"stuff\" + 0.004*\"imagine\"\n",
      "\n",
      "8: 0.048*\"rio\" + 0.012*\"city\" + 0.012*\"cable\" + 0.009*\"city_rio\" + 0.008*\"favela\" + 0.007*\"favelas\" + 0.005*\"performance_artist\" + 0.004*\"computer\" + 0.004*\"buoy\" + 0.003*\"blender\" + 0.003*\"elect_mayor\" + 0.003*\"music\" + 0.003*\"distorted\" + 0.003*\"art\" + 0.003*\"memorialize\" + 0.003*\"walk_aisle\" + 0.003*\"specialize\" + 0.003*\"possible\" + 0.002*\"oklahoma_city\" + 0.002*\"weather_data\"\n",
      "\n",
      "9: 0.021*\"woman\" + 0.009*\"child\" + 0.007*\"family\" + 0.006*\"men\" + 0.006*\"love\" + 0.005*\"never\" + 0.005*\"girl\" + 0.005*\"story\" + 0.005*\"kid\" + 0.005*\"man\" + 0.005*\"mother\" + 0.004*\"home\" + 0.004*\"help\" + 0.004*\"parent\" + 0.004*\"school\" + 0.003*\"feel\" + 0.003*\"country\" + 0.003*\"leave\" + 0.003*\"learn\" + 0.003*\"believe\"\n",
      "\n",
      "10: 0.043*\"brain\" + 0.033*\"sleep\" + 0.023*\"concussion\" + 0.013*\"beatboxing\" + 0.009*\"skull\" + 0.009*\"waste\" + 0.009*\"clear_away\" + 0.009*\"helmet\" + 0.009*\"douglas_fir\" + 0.008*\"parker\" + 0.007*\"dissection\" + 0.007*\"tree\" + 0.007*\"little_sister\" + 0.006*\"human_anatomy\" + 0.006*\"baobab\" + 0.006*\"dispensary\" + 0.006*\"video\" + 0.006*\"behind_bar\" + 0.005*\"lion\" + 0.004*\"imagery\"\n",
      "\n",
      "11: 0.028*\"power\" + 0.008*\"prison\" + 0.008*\"muslims\" + 0.007*\"muslim\" + 0.007*\"prisoner\" + 0.006*\"middle_east\" + 0.006*\"europe\" + 0.005*\"attack\" + 0.005*\"country\" + 0.005*\"iran\" + 0.005*\"terrorist\" + 0.005*\"islam\" + 0.005*\"story\" + 0.005*\"lebanon\" + 0.004*\"refugee\" + 0.004*\"report\" + 0.004*\"old\" + 0.004*\"mohammed\" + 0.004*\"value\" + 0.004*\"bomb\"\n",
      "\n",
      "12: 0.008*\"design\" + 0.006*\"book\" + 0.006*\"create\" + 0.005*\"computer\" + 0.005*\"technology\" + 0.005*\"sort\" + 0.004*\"kid\" + 0.004*\"project\" + 0.004*\"picture\" + 0.004*\"story\" + 0.003*\"food\" + 0.003*\"learn\" + 0.003*\"object\" + 0.003*\"build\" + 0.003*\"information\" + 0.003*\"grow\" + 0.003*\"data\" + 0.003*\"inside\" + 0.003*\"move\" + 0.003*\"might\"\n",
      "\n",
      "13: 0.021*\"plant\" + 0.015*\"animal\" + 0.011*\"specie\" + 0.010*\"dinosaur\" + 0.008*\"tree\" + 0.007*\"ant\" + 0.007*\"guerrilla\" + 0.006*\"camel\" + 0.006*\"bone\" + 0.006*\"audio\" + 0.006*\"male\" + 0.005*\"blockchain\" + 0.005*\"tapir\" + 0.005*\"insect\" + 0.004*\"human\" + 0.004*\"female\" + 0.004*\"seed\" + 0.004*\"gene\" + 0.004*\"water\" + 0.003*\"sex\"\n",
      "\n",
      "14: 0.009*\"robot\" + 0.005*\"might\" + 0.005*\"human\" + 0.004*\"help\" + 0.004*\"computer\" + 0.003*\"sort\" + 0.003*\"guy\" + 0.003*\"information\" + 0.003*\"point\" + 0.003*\"feel\" + 0.003*\"problem\" + 0.003*\"move\" + 0.003*\"probably\" + 0.003*\"system\" + 0.003*\"person\" + 0.002*\"case\" + 0.002*\"real\" + 0.002*\"control\" + 0.002*\"example\" + 0.002*\"maybe\"\n",
      "\n",
      "15: 0.010*\"choice\" + 0.010*\"country\" + 0.005*\"percent\" + 0.005*\"market\" + 0.005*\"number\" + 0.004*\"mexico\" + 0.004*\"save\" + 0.004*\"decision\" + 0.004*\"three\" + 0.004*\"problem\" + 0.004*\"canada\" + 0.003*\"choose\" + 0.003*\"maybe\" + 0.003*\"painting\" + 0.003*\"always\" + 0.003*\"best\" + 0.003*\"feel\" + 0.003*\"reason\" + 0.003*\"might\" + 0.003*\"buy\"\n",
      "\n",
      "16: 0.015*\"dance\" + 0.012*\"patent\" + 0.010*\"lead\" + 0.007*\"freedom\" + 0.007*\"case\" + 0.006*\"myriad\" + 0.006*\"judge\" + 0.006*\"follow\" + 0.005*\"name\" + 0.004*\"ivan\" + 0.004*\"switch\" + 0.004*\"chris\" + 0.004*\"test\" + 0.004*\"play\" + 0.004*\"middle_america\" + 0.004*\"lawyer\" + 0.004*\"justice\" + 0.003*\"court\" + 0.003*\"create\" + 0.003*\"together\"\n",
      "\n",
      "17: 0.007*\"human\" + 0.007*\"brain\" + 0.006*\"cell\" + 0.004*\"body\" + 0.004*\"able\" + 0.004*\"technology\" + 0.004*\"science\" + 0.004*\"might\" + 0.004*\"cancer\" + 0.004*\"disease\" + 0.004*\"example\" + 0.004*\"animal\" + 0.003*\"understand\" + 0.003*\"study\" + 0.003*\"gene\" + 0.003*\"maybe\" + 0.003*\"patient\" + 0.003*\"problem\" + 0.003*\"scientist\" + 0.003*\"learn\"\n",
      "\n",
      "18: 0.020*\"sound\" + 0.019*\"music\" + 0.017*\"word\" + 0.012*\"play\" + 0.011*\"language\" + 0.010*\"write\" + 0.010*\"hear\" + 0.006*\"example\" + 0.006*\"listen\" + 0.005*\"brain\" + 0.005*\"learn\" + 0.005*\"human\" + 0.004*\"child\" + 0.004*\"lie\" + 0.004*\"speak\" + 0.004*\"understand\" + 0.004*\"feel\" + 0.004*\"record\" + 0.004*\"hand\" + 0.003*\"video\"\n",
      "\n",
      "19: 0.008*\"city\" + 0.008*\"water\" + 0.008*\"build\" + 0.006*\"planet\" + 0.006*\"building\" + 0.005*\"space\" + 0.004*\"earth\" + 0.004*\"design\" + 0.004*\"create\" + 0.004*\"car\" + 0.004*\"light\" + 0.003*\"small\" + 0.003*\"system\" + 0.003*\"move\" + 0.003*\"able\" + 0.003*\"material\" + 0.003*\"form\" + 0.003*\"project\" + 0.003*\"area\" + 0.002*\"energy\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,topic in LDAModel.show_topics(formatted=True, num_topics = numberOfTopics, num_words=20):\n",
    "    print(str(i)+\": \"+ topic)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "file = open('models/LDAmodelExtended.pkl', 'wb')\n",
    "\n",
    "pkl.dump(LDAModel, file, protocol = pkl.DEFAULT_PROTOCOL)\n",
    "\n",
    "file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 0.58108824), (5, 0.06877321), (17, 0.04227818), (18, 0.30683613)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDAModel[bow[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
